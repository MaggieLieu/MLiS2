{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmw9UpzdikYSBha6N1Fyef",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/workshops/workshop5/rnn_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeUfbxv9ghmQ"
      },
      "source": [
        "In lectures we hand-coded the BPTT algorithm to train an RNN language model to predict the next word in a sentence.\n",
        "\n",
        "Using the same training corpus, train a many-to-many LSTM model using TF2 to perform the same task, and compare your results against a vanilla RNN.\n",
        "\n",
        "In this example we concatenate all the sentences into a single vector to make it easier to feed into the TF2 dataset API. We therefore only have a single stop word between sentences.\n",
        "\n",
        "The downside of this approach is that sentences in different reviews will have different context, so ideally we would treat different reviews separately and pad inputs where necessary. \n",
        "\n",
        "Tensorflow has a similar character level RNN (not at the word level) here to help you: https://www.tensorflow.org/tutorials/text/text_generation\n",
        "\n",
        "**NOTE: We do not attempt to implement regularisation here, so overfitting is likely an issue when training for a large number of epochs**\n",
        "\n",
        "**NOTE 2: You can decrease training time significantly by switching to a GPU instance on Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4VLQKu0ofFx"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alq1W_bCWoTM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDPfRGRmpHPN",
        "outputId": "9391d0a7-fd18-41b2-a5ff-fdf68f18f365"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhl7G_SQ7wlH"
      },
      "source": [
        "Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyc8euxWpzL"
      },
      "source": [
        "%%capture\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvLWBQt8EZ6"
      },
      "source": [
        "Upload imdb_sentences.txt file (or another file containing a list of sentences if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpGBFRj-DzH"
      },
      "source": [
        "if not os.path.isfile('imdb_sentences.txt'):\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHNYmTqB93G9"
      },
      "source": [
        "Add sentence start and end tags, convert to lower case and strip newlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBM3rWv9chG"
      },
      "source": [
        "sentence_start_token = \"SENTENCE_STOP\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QVY1IkAYEA"
      },
      "source": [
        "with open('imdb_sentences.txt', 'r') as f:\n",
        "  sentences = f.readlines()\n",
        "sentences = [\"%s %s\" % (sentence_start_token, x.lstrip().rstrip('.\\n').lower()) for x in sentences]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPIWW5lqtSLG",
        "outputId": "aaeba5a5-65d5-40a0-c571-e549162d79aa"
      },
      "source": [
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % sentences[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed 12188 sentences.\n",
            "Example: SENTENCE_STOP story of a man who has unnatural feelings for a pig\n",
            "Example: SENTENCE_STOP starts out with a opening scene that is a terrific example of absurd comedy\n",
            "Example: SENTENCE_STOP a formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers\n",
            "Example: SENTENCE_STOP unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting\n",
            "Example: SENTENCE_STOP even those from the era should be turned off\n",
            "Example: SENTENCE_STOP the cryptic dialogue would make shakespeare seem easy to a third grader\n",
            "Example: SENTENCE_STOP on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond\n",
            "Example: SENTENCE_STOP future stars sally kirkland and frederic forrest can be seen briefly\n",
            "Example: SENTENCE_STOP airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman philip stevens (james stewart) who is flying them & a bunch of vip's to his estate in preparation of it being opened to the public as a museum, also on board is stevens daughter julie (kathleen quinlan) & her son\n",
            "Example: SENTENCE_STOP the luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot chambers (robert foxworth) & his two accomplice's banker (monte markham) & wilson (michael pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBDO_fs-udT"
      },
      "source": [
        "Tokenize the sentences into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfoIQZV-tNL"
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDJ1NGJy-7i_",
        "outputId": "d89afdb6-baba-46c1-d871-931373ceb85a"
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18153 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGpHm2s_IyE"
      },
      "source": [
        "vocab_size = 1000\n",
        "unknown_token = 'UNKNOWN_TOKEN'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTJmo8G_Fv0"
      },
      "source": [
        "vocab = word_freq.most_common(vocab_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "index_to_word = np.array(index_to_word)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-C-wm5h_kQB"
      },
      "source": [
        "Replace all words not in our vocabulary with the unknown token and discard sentences under min / over max number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBo7x_Zpd77V"
      },
      "source": [
        "min_sentence_length = 5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhY2tX9duHzi"
      },
      "source": [
        "purged_sentences = []\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  if len(sent) >= min_sentence_length:\n",
        "    purged_sentences.append([w if w in word_to_index else unknown_token for w in sent])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHynO1W2x99x"
      },
      "source": [
        "Flatten sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXT2EKyw_h6p"
      },
      "source": [
        "text = [word for sent in purged_sentences for word in sent]\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMdDdrLlyBEx"
      },
      "source": [
        "Convert to integer representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r09ZMRxqk7pC"
      },
      "source": [
        "text_as_int = np.array([word_to_index[w] for w in text])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ7O4LNYyS5U"
      },
      "source": [
        "Set maximum length sentence we want for a single input in characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZy4RUuyOUm"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx-hXzf4_8g1"
      },
      "source": [
        "Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmy5ssHlwbfu"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDM9GOCzwuix"
      },
      "source": [
        "sequences = dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3NDYIojyJ5i"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XszttQKyyn3T",
        "outputId": "21f35660-4238-4a84-ec1d-a529633af9f7"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(' '.join(index_to_word[input_example.numpy()])))\n",
        "  print ('Target data:', repr(' '.join(index_to_word[target_example.numpy()])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"SENTENCE_STOP story of a man who has UNKNOWN_TOKEN UNKNOWN_TOKEN for a UNKNOWN_TOKEN SENTENCE_STOP starts out with a opening scene that is a UNKNOWN_TOKEN example of absurd comedy SENTENCE_STOP a UNKNOWN_TOKEN UNKNOWN_TOKEN audience is turned into an UNKNOWN_TOKEN , UNKNOWN_TOKEN UNKNOWN_TOKEN by the crazy UNKNOWN_TOKEN of it 's UNKNOWN_TOKEN SENTENCE_STOP unfortunately it UNKNOWN_TOKEN absurd the whole time with no general UNKNOWN_TOKEN eventually making it just too off UNKNOWN_TOKEN SENTENCE_STOP even those from the UNKNOWN_TOKEN should be turned off SENTENCE_STOP the UNKNOWN_TOKEN dialogue would make UNKNOWN_TOKEN seem UNKNOWN_TOKEN to a third UNKNOWN_TOKEN SENTENCE_STOP on a UNKNOWN_TOKEN level it 's better than you\"\n",
            "Target data: \"story of a man who has UNKNOWN_TOKEN UNKNOWN_TOKEN for a UNKNOWN_TOKEN SENTENCE_STOP starts out with a opening scene that is a UNKNOWN_TOKEN example of absurd comedy SENTENCE_STOP a UNKNOWN_TOKEN UNKNOWN_TOKEN audience is turned into an UNKNOWN_TOKEN , UNKNOWN_TOKEN UNKNOWN_TOKEN by the crazy UNKNOWN_TOKEN of it 's UNKNOWN_TOKEN SENTENCE_STOP unfortunately it UNKNOWN_TOKEN absurd the whole time with no general UNKNOWN_TOKEN eventually making it just too off UNKNOWN_TOKEN SENTENCE_STOP even those from the UNKNOWN_TOKEN should be turned off SENTENCE_STOP the UNKNOWN_TOKEN dialogue would make UNKNOWN_TOKEN seem UNKNOWN_TOKEN to a third UNKNOWN_TOKEN SENTENCE_STOP on a UNKNOWN_TOKEN level it 's better than you might\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_I6PqN7yrAQ"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nuPkFZr0-Ds"
      },
      "source": [
        "**Now code and train your RNN...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_E9upwd008X"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dry2o5xHj9wx"
      },
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihb6AmTdeReg"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim = embedding_dim,\n",
        "  rnn_units = rnn_units,\n",
        "  batch_size = BATCH_SIZE)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9hWVxGnj5WH"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3OqkVO9kU_U"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-GT4bBoYKk"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQq234H_kYV1"
      },
      "source": [
        "EPOCHS = 200"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRwVXw8zkdZF",
        "outputId": "5afbfa95-78cc-44e7-aff0-2665a3abfd23"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 12s 80ms/step - loss: 5.4556\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 4.7680\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 4.5541\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 4.2947\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 4.1355\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 4.0556\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 3.9831\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 4s 85ms/step - loss: 3.9088\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 4s 85ms/step - loss: 3.8615\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 4s 85ms/step - loss: 3.8004\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 4s 85ms/step - loss: 3.7497\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 3.7006\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 3.6513\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 3.6085\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.5645\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.5173\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.4780\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.4209\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.3753\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 3.3144\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 3.2669\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.2040\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.1395\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.0599\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 3.0082\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.9464\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.8576\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.8035\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.7274\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.6426\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.5876\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.4990\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.4367\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.3565\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.2710\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.2021\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.1456\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 2.0606\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 2.0185\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.9437\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.8997\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.8156\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.7614\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.7015\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.6256\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 1.5965\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.5344\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.4648\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.4357\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.3698\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.3117\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.2930\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.2529\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.1850\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 1.1655\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 1.1158\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 1.0687\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 1.0442\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 1.0097\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.9750\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.9455\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.9098\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.8827\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.8534\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.8327\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.8107\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.7830\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.7640\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.7341\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.7156\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.6939\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.6851\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.6589\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.6375\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.6252\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.6011\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5876\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5811\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.5612\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5544\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5468\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5270\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.5156\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.5044\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4975\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4807\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4752\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4644\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4578\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4527\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4422\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.4376\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4318\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4240\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4114\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4121\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.4024\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.4024\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3875\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3837\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3859\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3772\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3712\n",
            "Epoch 104/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3703\n",
            "Epoch 105/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3625\n",
            "Epoch 106/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3588\n",
            "Epoch 107/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3559\n",
            "Epoch 108/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.3511\n",
            "Epoch 109/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.3476\n",
            "Epoch 110/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3478\n",
            "Epoch 111/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3448\n",
            "Epoch 112/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3345\n",
            "Epoch 113/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3335\n",
            "Epoch 114/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3290\n",
            "Epoch 115/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3249\n",
            "Epoch 116/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3213\n",
            "Epoch 117/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3207\n",
            "Epoch 118/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3143\n",
            "Epoch 119/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3149\n",
            "Epoch 120/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.3086\n",
            "Epoch 121/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3042\n",
            "Epoch 122/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3077\n",
            "Epoch 123/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.3031\n",
            "Epoch 124/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.3010\n",
            "Epoch 125/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2938\n",
            "Epoch 126/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2948\n",
            "Epoch 127/200\n",
            "44/44 [==============================] - 4s 84ms/step - loss: 0.2894\n",
            "Epoch 128/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2886\n",
            "Epoch 129/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2825\n",
            "Epoch 130/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2846\n",
            "Epoch 131/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2811\n",
            "Epoch 132/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2834\n",
            "Epoch 133/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2788\n",
            "Epoch 134/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2747\n",
            "Epoch 135/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2735\n",
            "Epoch 136/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2710\n",
            "Epoch 137/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2697\n",
            "Epoch 138/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2691\n",
            "Epoch 139/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2656\n",
            "Epoch 140/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2648\n",
            "Epoch 141/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2622\n",
            "Epoch 142/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2586\n",
            "Epoch 143/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2573\n",
            "Epoch 144/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2536\n",
            "Epoch 145/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2526\n",
            "Epoch 146/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2498\n",
            "Epoch 147/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2462\n",
            "Epoch 148/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2466\n",
            "Epoch 149/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2465\n",
            "Epoch 150/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2431\n",
            "Epoch 151/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2421\n",
            "Epoch 152/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2390\n",
            "Epoch 153/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2372\n",
            "Epoch 154/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2407\n",
            "Epoch 155/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2383\n",
            "Epoch 156/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2387\n",
            "Epoch 157/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2377\n",
            "Epoch 158/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2371\n",
            "Epoch 159/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2332\n",
            "Epoch 160/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2274\n",
            "Epoch 161/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2282\n",
            "Epoch 162/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2283\n",
            "Epoch 163/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2257\n",
            "Epoch 164/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2243\n",
            "Epoch 165/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2232\n",
            "Epoch 166/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2202\n",
            "Epoch 167/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2207\n",
            "Epoch 168/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2195\n",
            "Epoch 169/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2186\n",
            "Epoch 170/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2179\n",
            "Epoch 171/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2171\n",
            "Epoch 172/200\n",
            "44/44 [==============================] - 4s 81ms/step - loss: 0.2156\n",
            "Epoch 173/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2165\n",
            "Epoch 174/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2116\n",
            "Epoch 175/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2128\n",
            "Epoch 176/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2112\n",
            "Epoch 177/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2119\n",
            "Epoch 178/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2096\n",
            "Epoch 179/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2080\n",
            "Epoch 180/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2054\n",
            "Epoch 181/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2043\n",
            "Epoch 182/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2053\n",
            "Epoch 183/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2026\n",
            "Epoch 184/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.2031\n",
            "Epoch 185/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2008\n",
            "Epoch 186/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.2020\n",
            "Epoch 187/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.1996\n",
            "Epoch 188/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1970\n",
            "Epoch 189/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1988\n",
            "Epoch 190/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1969\n",
            "Epoch 191/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.1998\n",
            "Epoch 192/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1943\n",
            "Epoch 193/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.1940\n",
            "Epoch 194/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1943\n",
            "Epoch 195/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.1910\n",
            "Epoch 196/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1924\n",
            "Epoch 197/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1915\n",
            "Epoch 198/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1913\n",
            "Epoch 199/200\n",
            "44/44 [==============================] - 4s 83ms/step - loss: 0.1897\n",
            "Epoch 200/200\n",
            "44/44 [==============================] - 4s 82ms/step - loss: 0.1882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivypVshhkhl5"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkKiKWEiqNRE",
        "outputId": "2d1d42bc-8015-4605-b9be-f32486332481"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f729805f310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4MclNSqRiF"
      },
      "source": [
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5NXlrW7qT_H",
        "outputId": "1aad75f9-e8e9-49ba-a326-f779249e9daa"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            256000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 1000)           1025000   \n",
            "=================================================================\n",
            "Total params: 6,527,976\n",
            "Trainable params: 6,527,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-aIFyZ_sag6"
      },
      "source": [
        "unknown_index = word_to_index['UNKNOWN_TOKEN']"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcvcq1VVqWBY"
      },
      "source": [
        "def generate_text(model, start_word):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 30\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [word_to_index[start_word]]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      if predicted_id != unknown_index:\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index_to_word[predicted_id])\n",
        "\n",
        "  sentence = ' '.join(text_generated)\n",
        "  sentence = sentence.replace('SENTENCE_STOP', '. ')\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh2FTk-gqbe9",
        "outputId": "3e6df8b4-b946-4602-d17e-55273c2f0415"
      },
      "source": [
        "for i in range(5):\n",
        "  print(generate_text(model, start_word=u\"SENTENCE_STOP\"))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anyway major plot had little interest in the movie even sure because it was shown .  an opportunity wasted by no one to be a good\n",
            "end your minute of any kind of lame have very weird for great , but i 'd be so not see that tom and\n",
            "not worth fun and it just goes on their family .  they could n't get back along in the movie and what i 'm .  with ,\n",
            "otherwise played by the way it certainly is worth is because of some sort of that time , effort and is\n",
            "not worth am enough that they could n't even tell ends them , because it was a disappointment .  the human race , especially where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACKmHh7rqfGc"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}