{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9lgu7n44eFs"
   },
   "source": [
    "**Regularisation**\n",
    "\n",
    "In this example you are given a dataset (MNIST) with a limited number of training examples (only 1000 compared to the usual 60,000). \n",
    "\n",
    "Your goal is to implement regularisation methods to achive the **lowest possible test loss using this dataset**. \n",
    "\n",
    "You should consider methods given in the lectures including:\n",
    "\n",
    "*   Data augmentation\n",
    "*   Early stopping\n",
    "*   L1/L2 penalty norms\n",
    "*   Dropout\n",
    "\n",
    "You are free to change the network architecture and model complexity, but the main purpose of the workshop is to investigate regularisation (next week you will look at CNN architectures in detail). \n",
    "\n",
    "You are also free to change the choice of optimiser, and other hyper-parameters such as the batch size.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cNviuTwum6vs"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ARZRYjHQnE-0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7I-MxTxAnH2d"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxO576eJnMXN",
    "outputId": "eb98d659-c10a-45a9-c0de-cd1d5fc39661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4F_HTQpnvSk"
   },
   "source": [
    "First load the MNIST dataset and add a channels dimension (channels last convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Twdc-t9FnN_9",
    "outputId": "80fdfa09-b62f-4beb-d947-c1430384a70a"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train[..., tf.newaxis].astype(np.float32)\n",
    "x_test = x_test[..., tf.newaxis].astype(np.float32)\n",
    "\n",
    "img_rows = x_train.shape[1]\n",
    "img_cols = x_train.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEsWYkNz5Po7"
   },
   "source": [
    "Let's use a much smaller training dataset of 1000 examples so overfitting is more problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qzt-HoIo4yks",
    "outputId": "1ed26243-ec3b-4965-fb7d-2633044cc9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "n_train = 1000\n",
    "x_train = x_train[0:n_train, :]\n",
    "y_train = y_train[0:n_train]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x2JSWYGKjsPh"
   },
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(np.squeeze(img))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img):\n",
    "    '''Add random noise to an image'''\n",
    "    VARIABILITY = 35\n",
    "    deviation = VARIABILITY*random.random()\n",
    "    noise = np.random.normal(0, deviation, img.shape)\n",
    "    img += noise\n",
    "    np.clip(img, 0., 255.)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDfQh4AfoBAH"
   },
   "source": [
    "Let's visualise several training examples - to do this we use the keras ImageDataGenerator. We rescale images by 1/255 to normalise them in the range (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "fuOz9BopjZPq",
    "outputId": "6a3dab12-edb5-4776-b3ad-b08574dc69e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_generator = ImageDataGenerator(rescale=1./255) \n",
    "data_gen = image_generator.flow(x_train, y_train, batch_size=32) \n",
    "sample_images, sample_labels = next(data_gen)\n",
    "plotImages(sample_images[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8bStvJVoVlj"
   },
   "source": [
    "One regularisation method to deal with over-fitting is data augmentation. The image generator can apply various transformations to data - here we apply a random rotation of upto 20 degrees and visualise the same training example with different augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "iz28zlncmIiP",
    "outputId": "080d550f-ae72-447e-deed-c5d989a8ca97"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     rotation_range=20) \n",
    "data_gen = image_generator.flow(x_train, y_train, batch_size=32) \n",
    "augmented_images = [data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIE6-wlA3J4h"
   },
   "source": [
    "Define a basic CNN with 32 convolutional filters using a 3x3 kernel, followed by a dense fully connected layer of 128 units and an output layer of 10 units with softmax activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ZKolHZQUL8-4"
   },
   "outputs": [],
   "source": [
    "class BasicCNN(Model):\n",
    "  def __init__(self):\n",
    "    super(BasicCNN, self).__init__()\n",
    "    l = 0.0001\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(l))\n",
    "    self.conv2 = Conv2D(32, 3, activation='relu')\n",
    "    self.conv3 = Conv2D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(l))\n",
    "    self.drop1 = Dropout(0.2)\n",
    "    self.flatten = Flatten()\n",
    "    self.batch1 = BatchNormalization()\n",
    "    self.d1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l))\n",
    "    self.d2 = Dense(50, activation='relu')\n",
    "    self.batch2 = BatchNormalization()\n",
    "    self.d3 = Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(l))\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.drop1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.batch1(x)\n",
    "    x = self.d1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.batch2(x)\n",
    "    return self.d3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug = x_train\n",
    "y_train_aug = y_train\n",
    "for i in range(49):\n",
    "    x_train_aug = np.concatenate((x_train_aug, x_train), axis = 0)\n",
    "    y_train_aug = np.concatenate((y_train_aug, y_train), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuhNDi134B2G"
   },
   "source": [
    "Main training routine - uses the more detailed Gradient Tape API to iterate over the dataset and update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "MZH8jdqKohiU"
   },
   "outputs": [],
   "source": [
    "def trainer(cls, train_image_generator, test_image_generator, \n",
    "            verbose=False, batch_size=32, max_epochs=5):\n",
    "  \n",
    "  model = cls()\n",
    "\n",
    "  train_data_gen = train_image_generator.flow(x_train_aug, y_train_aug, \n",
    "                                              batch_size=batch_size) \n",
    "\n",
    "  test_data_gen = test_image_generator.flow(x_test, y_test, \n",
    "                                              batch_size=batch_size) \n",
    "\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(images, training=True)\n",
    "      loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    batches = 0\n",
    "    for images, labels in train_data_gen:\n",
    "      #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "      train_step(images, labels)\n",
    "      batches += 1\n",
    "      if batches >= len(x_train) / batch_size:\n",
    "        break\n",
    "\n",
    "    batches = 0\n",
    "    for images, labels in test_data_gen:\n",
    "      test_step(images, labels)\n",
    "      batches += 1\n",
    "      if batches >= len(x_test) / batch_size:\n",
    "        break\n",
    "\n",
    "    if verbose:\n",
    "      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "      print(template.format(epoch+1,\n",
    "                            train_loss.result(),\n",
    "                            train_accuracy.result()*100,\n",
    "                            test_loss.result(),\n",
    "                            test_accuracy.result()*100))\n",
    "    \n",
    "  return test_loss.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBSBEZftgDsL"
   },
   "source": [
    "Baseline run with no regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1shf8axytmnX"
   },
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.542130947113037, Accuracy: 14.84375, Test Loss: 2.3133046627044678, Test Accuracy: 13.429999351501465\n",
      "Epoch 2, Loss: 2.279090642929077, Accuracy: 19.04296875, Test Loss: 2.2676265239715576, Test Accuracy: 12.819999694824219\n",
      "Epoch 3, Loss: 2.1403589248657227, Accuracy: 24.51171875, Test Loss: 2.222956657409668, Test Accuracy: 10.65999984741211\n",
      "Epoch 4, Loss: 2.0712990760803223, Accuracy: 27.9296875, Test Loss: 2.15893816947937, Test Accuracy: 16.94999885559082\n",
      "Epoch 5, Loss: 1.9543068408966064, Accuracy: 32.8125, Test Loss: 2.143716335296631, Test Accuracy: 20.96000099182129\n",
      "Epoch 6, Loss: 1.8476574420928955, Accuracy: 34.765625, Test Loss: 2.114906072616577, Test Accuracy: 21.389999389648438\n",
      "Epoch 7, Loss: 1.7578355073928833, Accuracy: 41.50390625, Test Loss: 2.1159555912017822, Test Accuracy: 35.26000213623047\n",
      "Epoch 8, Loss: 1.673598051071167, Accuracy: 41.89453125, Test Loss: 2.0595388412475586, Test Accuracy: 27.329999923706055\n",
      "Epoch 9, Loss: 1.6042768955230713, Accuracy: 44.62890625, Test Loss: 2.04426646232605, Test Accuracy: 29.6299991607666\n",
      "Epoch 10, Loss: 1.5450748205184937, Accuracy: 47.94921875, Test Loss: 2.0784988403320312, Test Accuracy: 27.31999969482422\n",
      "Epoch 11, Loss: 1.45907461643219, Accuracy: 50.5859375, Test Loss: 1.928614854812622, Test Accuracy: 36.09000015258789\n",
      "Epoch 12, Loss: 1.3593004941940308, Accuracy: 55.76171875, Test Loss: 1.9468013048171997, Test Accuracy: 39.189998626708984\n",
      "Epoch 13, Loss: 1.3235293626785278, Accuracy: 60.44921875, Test Loss: 1.7297775745391846, Test Accuracy: 51.95000076293945\n",
      "Epoch 14, Loss: 1.2778476476669312, Accuracy: 58.10546875, Test Loss: 1.8643938302993774, Test Accuracy: 40.900001525878906\n",
      "Epoch 15, Loss: 1.2599985599517822, Accuracy: 58.59375, Test Loss: 1.6822054386138916, Test Accuracy: 43.15999984741211\n",
      "Epoch 16, Loss: 1.146454095840454, Accuracy: 63.76953125, Test Loss: 1.719482660293579, Test Accuracy: 41.310001373291016\n",
      "Epoch 17, Loss: 1.1022822856903076, Accuracy: 64.74609375, Test Loss: 1.6135804653167725, Test Accuracy: 49.77000045776367\n",
      "Epoch 18, Loss: 1.1300585269927979, Accuracy: 63.57421875, Test Loss: 1.5320022106170654, Test Accuracy: 56.19999694824219\n",
      "Epoch 19, Loss: 1.0455385446548462, Accuracy: 68.26171875, Test Loss: 1.6861258745193481, Test Accuracy: 45.779998779296875\n",
      "Epoch 20, Loss: 1.0345101356506348, Accuracy: 66.015625, Test Loss: 1.2630259990692139, Test Accuracy: 65.1500015258789\n",
      "Epoch 21, Loss: 1.0136592388153076, Accuracy: 67.48046875, Test Loss: 1.2002458572387695, Test Accuracy: 69.9000015258789\n",
      "Epoch 22, Loss: 0.9442505836486816, Accuracy: 69.7265625, Test Loss: 1.3146700859069824, Test Accuracy: 64.81000518798828\n",
      "Epoch 23, Loss: 0.9832634925842285, Accuracy: 68.75, Test Loss: 1.5513780117034912, Test Accuracy: 57.76000213623047\n",
      "Epoch 24, Loss: 0.9353137016296387, Accuracy: 70.3125, Test Loss: 1.1146574020385742, Test Accuracy: 66.47999572753906\n",
      "Epoch 25, Loss: 0.8700754046440125, Accuracy: 73.6328125, Test Loss: 0.998613715171814, Test Accuracy: 72.93999481201172\n",
      "Epoch 26, Loss: 0.804801344871521, Accuracy: 73.92578125, Test Loss: 0.8498275876045227, Test Accuracy: 80.5\n",
      "Epoch 27, Loss: 0.8388392925262451, Accuracy: 75.78125, Test Loss: 0.838797390460968, Test Accuracy: 78.29000091552734\n",
      "Epoch 28, Loss: 0.8063681125640869, Accuracy: 73.14453125, Test Loss: 0.6616290211677551, Test Accuracy: 87.66000366210938\n",
      "Epoch 29, Loss: 0.7645570039749146, Accuracy: 77.34375, Test Loss: 0.6110486388206482, Test Accuracy: 85.31999969482422\n",
      "Epoch 30, Loss: 0.8008197546005249, Accuracy: 74.30555725097656, Test Loss: 0.5968731045722961, Test Accuracy: 86.36000061035156\n",
      "Epoch 31, Loss: 0.817722499370575, Accuracy: 75.48828125, Test Loss: 0.6072522401809692, Test Accuracy: 85.97999572753906\n",
      "Epoch 32, Loss: 0.77408766746521, Accuracy: 75.0, Test Loss: 0.4523347020149231, Test Accuracy: 91.70999908447266\n",
      "Epoch 33, Loss: 0.7697163820266724, Accuracy: 75.09765625, Test Loss: 0.4151913523674011, Test Accuracy: 91.08000183105469\n",
      "Epoch 34, Loss: 0.7194610238075256, Accuracy: 78.02734375, Test Loss: 0.46077990531921387, Test Accuracy: 88.80000305175781\n",
      "Epoch 35, Loss: 0.7345951795578003, Accuracy: 76.953125, Test Loss: 0.46833500266075134, Test Accuracy: 86.41999816894531\n",
      "Epoch 36, Loss: 0.6687356233596802, Accuracy: 78.22265625, Test Loss: 0.29027068614959717, Test Accuracy: 92.33999633789062\n",
      "Epoch 37, Loss: 0.6912561655044556, Accuracy: 78.515625, Test Loss: 0.26675039529800415, Test Accuracy: 92.87999725341797\n",
      "Epoch 38, Loss: 0.7149131894111633, Accuracy: 78.90625, Test Loss: 0.2533283233642578, Test Accuracy: 92.94000244140625\n",
      "Epoch 39, Loss: 0.6791479587554932, Accuracy: 78.3203125, Test Loss: 0.29030534625053406, Test Accuracy: 90.44000244140625\n",
      "Epoch 40, Loss: 0.6714238524436951, Accuracy: 79.4921875, Test Loss: 0.3341663181781769, Test Accuracy: 88.9000015258789\n",
      "Epoch 41, Loss: 0.6930942535400391, Accuracy: 77.24609375, Test Loss: 0.24736295640468597, Test Accuracy: 93.22000122070312\n",
      "Epoch 42, Loss: 0.6553871035575867, Accuracy: 80.17578125, Test Loss: 0.24165213108062744, Test Accuracy: 92.4000015258789\n",
      "Epoch 43, Loss: 0.6214100122451782, Accuracy: 79.8828125, Test Loss: 0.2458002120256424, Test Accuracy: 92.63999938964844\n",
      "Epoch 44, Loss: 0.6329313516616821, Accuracy: 81.0546875, Test Loss: 0.271170049905777, Test Accuracy: 91.86000061035156\n",
      "Epoch 45, Loss: 0.6378157734870911, Accuracy: 80.46875, Test Loss: 0.23868703842163086, Test Accuracy: 92.63999938964844\n",
      "Epoch 46, Loss: 0.6575794219970703, Accuracy: 79.58984375, Test Loss: 0.3664762079715729, Test Accuracy: 89.06000518798828\n",
      "Epoch 47, Loss: 0.6608267426490784, Accuracy: 79.4921875, Test Loss: 0.21873196959495544, Test Accuracy: 93.58999633789062\n",
      "Epoch 48, Loss: 0.6289938688278198, Accuracy: 80.17578125, Test Loss: 0.2635040283203125, Test Accuracy: 91.31999969482422\n",
      "Epoch 49, Loss: 0.6021249294281006, Accuracy: 81.4453125, Test Loss: 0.30205976963043213, Test Accuracy: 90.68000030517578\n",
      "Epoch 50, Loss: 0.5493261218070984, Accuracy: 82.421875, Test Loss: 0.24611540138721466, Test Accuracy: 92.62999725341797\n",
      "Epoch 51, Loss: 0.5437357425689697, Accuracy: 82.12890625, Test Loss: 0.22492609918117523, Test Accuracy: 93.30000305175781\n",
      "Epoch 52, Loss: 0.5450614094734192, Accuracy: 83.10546875, Test Loss: 0.2150251567363739, Test Accuracy: 93.37999725341797\n",
      "Epoch 53, Loss: 0.5522246956825256, Accuracy: 83.203125, Test Loss: 0.41504624485969543, Test Accuracy: 87.9800033569336\n",
      "Epoch 54, Loss: 0.5762985944747925, Accuracy: 82.2265625, Test Loss: 0.602618396282196, Test Accuracy: 84.65999603271484\n",
      "Epoch 55, Loss: 0.604953408241272, Accuracy: 80.46875, Test Loss: 0.31472277641296387, Test Accuracy: 90.05000305175781\n",
      "Epoch 56, Loss: 0.580514132976532, Accuracy: 81.54296875, Test Loss: 0.1992293894290924, Test Accuracy: 93.70999908447266\n",
      "Epoch 57, Loss: 0.5751164555549622, Accuracy: 82.2265625, Test Loss: 0.23486192524433136, Test Accuracy: 92.69000244140625\n",
      "Epoch 58, Loss: 0.5690659284591675, Accuracy: 82.8125, Test Loss: 0.4611084461212158, Test Accuracy: 85.7699966430664\n",
      "Epoch 59, Loss: 0.5271077156066895, Accuracy: 83.53174591064453, Test Loss: 0.24817194044589996, Test Accuracy: 92.56999969482422\n",
      "Epoch 60, Loss: 0.5041027665138245, Accuracy: 84.9609375, Test Loss: 0.33795642852783203, Test Accuracy: 90.37000274658203\n",
      "Epoch 61, Loss: 0.5478794574737549, Accuracy: 81.0546875, Test Loss: 0.2948114573955536, Test Accuracy: 90.69999694824219\n",
      "Epoch 62, Loss: 0.4611254632472992, Accuracy: 84.66796875, Test Loss: 0.18065685033798218, Test Accuracy: 94.45999908447266\n",
      "Epoch 63, Loss: 0.5340715050697327, Accuracy: 83.3984375, Test Loss: 0.23225811123847961, Test Accuracy: 93.05999755859375\n",
      "Epoch 64, Loss: 0.510976254940033, Accuracy: 84.27734375, Test Loss: 0.23488062620162964, Test Accuracy: 92.44999694824219\n",
      "Epoch 65, Loss: 0.5273147225379944, Accuracy: 83.984375, Test Loss: 0.17413291335105896, Test Accuracy: 94.51000213623047\n",
      "Epoch 66, Loss: 0.4784812033176422, Accuracy: 84.5703125, Test Loss: 0.1750747710466385, Test Accuracy: 94.59000396728516\n",
      "Epoch 67, Loss: 0.5094918012619019, Accuracy: 82.8125, Test Loss: 0.29694461822509766, Test Accuracy: 91.50999450683594\n",
      "Epoch 68, Loss: 0.5029972791671753, Accuracy: 85.15625, Test Loss: 0.2777368724346161, Test Accuracy: 91.68000030517578\n",
      "Epoch 69, Loss: 0.5162912607192993, Accuracy: 84.08203125, Test Loss: 0.2134382575750351, Test Accuracy: 93.51000213623047\n",
      "Epoch 70, Loss: 0.5398662090301514, Accuracy: 83.7890625, Test Loss: 0.1817457228899002, Test Accuracy: 94.5999984741211\n",
      "Epoch 71, Loss: 0.45273861289024353, Accuracy: 85.44921875, Test Loss: 0.26519203186035156, Test Accuracy: 92.19999694824219\n",
      "Epoch 72, Loss: 0.5306323170661926, Accuracy: 83.88671875, Test Loss: 0.2239697426557541, Test Accuracy: 93.48999786376953\n",
      "Epoch 73, Loss: 0.4747645854949951, Accuracy: 85.44921875, Test Loss: 0.2004251778125763, Test Accuracy: 94.06999969482422\n",
      "Epoch 74, Loss: 0.5026542544364929, Accuracy: 84.5703125, Test Loss: 0.23520232737064362, Test Accuracy: 92.81999969482422\n",
      "Epoch 75, Loss: 0.4959834814071655, Accuracy: 84.1796875, Test Loss: 0.43335816264152527, Test Accuracy: 88.4000015258789\n",
      "Epoch 76, Loss: 0.5872508883476257, Accuracy: 82.32421875, Test Loss: 0.22642655670642853, Test Accuracy: 92.94999694824219\n",
      "Epoch 77, Loss: 0.49170440435409546, Accuracy: 85.44921875, Test Loss: 0.18291476368904114, Test Accuracy: 94.30999755859375\n",
      "Epoch 78, Loss: 0.4735739827156067, Accuracy: 84.1796875, Test Loss: 0.23088833689689636, Test Accuracy: 92.90999603271484\n",
      "Epoch 79, Loss: 0.45230796933174133, Accuracy: 85.44921875, Test Loss: 0.19102220237255096, Test Accuracy: 94.12999725341797\n",
      "Epoch 80, Loss: 0.47005918622016907, Accuracy: 85.546875, Test Loss: 0.3467751145362854, Test Accuracy: 90.10000610351562\n",
      "Epoch 81, Loss: 0.4240957200527191, Accuracy: 85.9375, Test Loss: 0.26177456974983215, Test Accuracy: 92.47999572753906\n",
      "Epoch 82, Loss: 0.47892940044403076, Accuracy: 86.1328125, Test Loss: 0.16103360056877136, Test Accuracy: 95.06000518798828\n",
      "Epoch 83, Loss: 0.5035827159881592, Accuracy: 83.3984375, Test Loss: 0.21154357492923737, Test Accuracy: 93.73999786376953\n",
      "Epoch 84, Loss: 0.49280864000320435, Accuracy: 84.47265625, Test Loss: 0.15556634962558746, Test Accuracy: 95.4000015258789\n",
      "Epoch 85, Loss: 0.43843427300453186, Accuracy: 86.1328125, Test Loss: 0.21338330209255219, Test Accuracy: 93.83999633789062\n",
      "Epoch 86, Loss: 0.46799126267433167, Accuracy: 85.7421875, Test Loss: 0.1860283464193344, Test Accuracy: 94.81999969482422\n",
      "Epoch 87, Loss: 0.42847388982772827, Accuracy: 86.328125, Test Loss: 0.21660873293876648, Test Accuracy: 93.86000061035156\n",
      "Epoch 88, Loss: 0.4275386929512024, Accuracy: 87.20238494873047, Test Loss: 0.27855032682418823, Test Accuracy: 92.13999938964844\n",
      "Epoch 89, Loss: 0.4408205449581146, Accuracy: 85.546875, Test Loss: 0.3717947006225586, Test Accuracy: 89.3800048828125\n",
      "Epoch 90, Loss: 0.4262601137161255, Accuracy: 86.9140625, Test Loss: 0.19637882709503174, Test Accuracy: 94.0199966430664\n",
      "Epoch 91, Loss: 0.4463096261024475, Accuracy: 85.64453125, Test Loss: 0.19254688918590546, Test Accuracy: 94.22000122070312\n",
      "Epoch 92, Loss: 0.4692009687423706, Accuracy: 85.15625, Test Loss: 0.16471411287784576, Test Accuracy: 95.16000366210938\n",
      "Epoch 93, Loss: 0.3979763090610504, Accuracy: 87.3046875, Test Loss: 0.18562759459018707, Test Accuracy: 93.62000274658203\n",
      "Epoch 94, Loss: 0.38155442476272583, Accuracy: 87.890625, Test Loss: 0.16097019612789154, Test Accuracy: 94.81999969482422\n",
      "Epoch 95, Loss: 0.40744251012802124, Accuracy: 87.109375, Test Loss: 0.13525007665157318, Test Accuracy: 95.86000061035156\n",
      "Epoch 96, Loss: 0.4441990852355957, Accuracy: 85.7421875, Test Loss: 0.20872116088867188, Test Accuracy: 94.09000396728516\n",
      "Epoch 97, Loss: 0.440574586391449, Accuracy: 85.83984375, Test Loss: 0.18646234273910522, Test Accuracy: 94.59000396728516\n",
      "Epoch 98, Loss: 0.4593299329280853, Accuracy: 84.5703125, Test Loss: 0.17315208911895752, Test Accuracy: 95.06999969482422\n",
      "Epoch 99, Loss: 0.3995429277420044, Accuracy: 87.40234375, Test Loss: 0.17285409569740295, Test Accuracy: 95.12000274658203\n",
      "Epoch 100, Loss: 0.3707038462162018, Accuracy: 88.57421875, Test Loss: 0.2902068495750427, Test Accuracy: 91.68999481201172\n",
      "Epoch 101, Loss: 0.4830409288406372, Accuracy: 84.5703125, Test Loss: 0.19511626660823822, Test Accuracy: 93.94999694824219\n",
      "Epoch 102, Loss: 0.4145909547805786, Accuracy: 87.40234375, Test Loss: 0.17032849788665771, Test Accuracy: 94.7300033569336\n",
      "Epoch 103, Loss: 0.4020416736602783, Accuracy: 87.5, Test Loss: 0.1815822571516037, Test Accuracy: 94.34000396728516\n",
      "Epoch 104, Loss: 0.434126079082489, Accuracy: 85.9375, Test Loss: 0.16490043699741364, Test Accuracy: 95.04000091552734\n",
      "Epoch 105, Loss: 0.4399319887161255, Accuracy: 86.42578125, Test Loss: 0.1449616402387619, Test Accuracy: 95.44000244140625\n",
      "Epoch 106, Loss: 0.35298359394073486, Accuracy: 88.96484375, Test Loss: 0.13925069570541382, Test Accuracy: 95.56999969482422\n",
      "Epoch 107, Loss: 0.4173985719680786, Accuracy: 87.3046875, Test Loss: 0.15144050121307373, Test Accuracy: 95.3499984741211\n",
      "Epoch 108, Loss: 0.41107240319252014, Accuracy: 86.9140625, Test Loss: 0.18662317097187042, Test Accuracy: 94.6500015258789\n",
      "Epoch 109, Loss: 0.43866339325904846, Accuracy: 85.9375, Test Loss: 0.17958827316761017, Test Accuracy: 94.5999984741211\n",
      "Epoch 110, Loss: 0.39645564556121826, Accuracy: 87.109375, Test Loss: 0.16736632585525513, Test Accuracy: 95.06999969482422\n",
      "Epoch 111, Loss: 0.4400000274181366, Accuracy: 86.5234375, Test Loss: 0.22008799016475677, Test Accuracy: 93.93000030517578\n",
      "Epoch 112, Loss: 0.46270400285720825, Accuracy: 84.9609375, Test Loss: 0.18841026723384857, Test Accuracy: 94.27999877929688\n",
      "Epoch 113, Loss: 0.42451226711273193, Accuracy: 87.01171875, Test Loss: 0.16747406125068665, Test Accuracy: 95.20999908447266\n",
      "Epoch 114, Loss: 0.38465574383735657, Accuracy: 87.59765625, Test Loss: 0.2597648799419403, Test Accuracy: 92.22000122070312\n",
      "Epoch 115, Loss: 0.3706052005290985, Accuracy: 88.8671875, Test Loss: 0.15860706567764282, Test Accuracy: 95.31000518798828\n",
      "Epoch 116, Loss: 0.3659361004829407, Accuracy: 89.16015625, Test Loss: 0.17169639468193054, Test Accuracy: 95.26000213623047\n",
      "Epoch 117, Loss: 0.39930295944213867, Accuracy: 87.01171875, Test Loss: 0.15814924240112305, Test Accuracy: 95.7300033569336\n",
      "Epoch 118, Loss: 0.4165681302547455, Accuracy: 86.70634460449219, Test Loss: 0.13889476656913757, Test Accuracy: 95.95000457763672\n",
      "Epoch 119, Loss: 0.3919297456741333, Accuracy: 87.40234375, Test Loss: 0.18882432579994202, Test Accuracy: 94.45000457763672\n",
      "Epoch 120, Loss: 0.36412984132766724, Accuracy: 88.28125, Test Loss: 0.15533068776130676, Test Accuracy: 95.31000518798828\n",
      "Epoch 121, Loss: 0.3766472637653351, Accuracy: 87.98828125, Test Loss: 0.13897886872291565, Test Accuracy: 96.17000579833984\n",
      "Epoch 122, Loss: 0.3643158972263336, Accuracy: 88.28125, Test Loss: 0.2099764347076416, Test Accuracy: 94.06999969482422\n",
      "Epoch 123, Loss: 0.36024045944213867, Accuracy: 88.28125, Test Loss: 0.1968946009874344, Test Accuracy: 94.62000274658203\n",
      "Epoch 124, Loss: 0.3800572156906128, Accuracy: 88.4765625, Test Loss: 0.283877432346344, Test Accuracy: 91.5199966430664\n",
      "Epoch 125, Loss: 0.4010041058063507, Accuracy: 86.71875, Test Loss: 0.24375106394290924, Test Accuracy: 93.66999816894531\n",
      "Epoch 126, Loss: 0.44149497151374817, Accuracy: 85.3515625, Test Loss: 0.13712486624717712, Test Accuracy: 95.8800048828125\n",
      "Epoch 127, Loss: 0.3504520654678345, Accuracy: 89.2578125, Test Loss: 0.1575709730386734, Test Accuracy: 95.13999938964844\n",
      "Epoch 128, Loss: 0.34207844734191895, Accuracy: 88.4765625, Test Loss: 0.16256015002727509, Test Accuracy: 95.31999969482422\n",
      "Epoch 129, Loss: 0.34230297803878784, Accuracy: 88.76953125, Test Loss: 0.1272074282169342, Test Accuracy: 96.11000061035156\n",
      "Epoch 130, Loss: 0.3999122083187103, Accuracy: 85.83984375, Test Loss: 0.1248062252998352, Test Accuracy: 96.0300064086914\n",
      "Epoch 131, Loss: 0.4093029201030731, Accuracy: 88.4765625, Test Loss: 0.1707708239555359, Test Accuracy: 94.7300033569336\n",
      "Epoch 132, Loss: 0.3593146800994873, Accuracy: 88.8671875, Test Loss: 0.13510581851005554, Test Accuracy: 96.01000213623047\n",
      "Epoch 133, Loss: 0.3266812562942505, Accuracy: 90.72265625, Test Loss: 0.13880185782909393, Test Accuracy: 95.80000305175781\n",
      "Epoch 134, Loss: 0.3462250828742981, Accuracy: 89.74609375, Test Loss: 0.21446755528450012, Test Accuracy: 93.69000244140625\n",
      "Epoch 135, Loss: 0.40314507484436035, Accuracy: 87.01171875, Test Loss: 0.12043672055006027, Test Accuracy: 96.1300048828125\n",
      "Epoch 136, Loss: 0.40289434790611267, Accuracy: 87.109375, Test Loss: 0.15787717700004578, Test Accuracy: 95.36000061035156\n",
      "Epoch 137, Loss: 0.3630300760269165, Accuracy: 88.37890625, Test Loss: 0.15033656358718872, Test Accuracy: 95.4800033569336\n",
      "Epoch 138, Loss: 0.34626227617263794, Accuracy: 88.96484375, Test Loss: 0.13435998558998108, Test Accuracy: 96.10000610351562\n",
      "Epoch 139, Loss: 0.36383870244026184, Accuracy: 88.37890625, Test Loss: 0.13122713565826416, Test Accuracy: 95.8800048828125\n",
      "Epoch 140, Loss: 0.3499613106250763, Accuracy: 88.4765625, Test Loss: 0.16651803255081177, Test Accuracy: 95.2300033569336\n",
      "Epoch 141, Loss: 0.3514414131641388, Accuracy: 89.35546875, Test Loss: 0.1526087075471878, Test Accuracy: 95.56000518798828\n",
      "Epoch 142, Loss: 0.3534511625766754, Accuracy: 88.57421875, Test Loss: 0.14412397146224976, Test Accuracy: 95.77999877929688\n",
      "Epoch 143, Loss: 0.36048343777656555, Accuracy: 88.96484375, Test Loss: 0.13377897441387177, Test Accuracy: 96.0\n",
      "Epoch 144, Loss: 0.38385245203971863, Accuracy: 88.28125, Test Loss: 0.15415863692760468, Test Accuracy: 95.61000061035156\n",
      "Epoch 145, Loss: 0.3434029221534729, Accuracy: 89.74609375, Test Loss: 0.33570507168769836, Test Accuracy: 90.72999572753906\n",
      "Epoch 146, Loss: 0.38102906942367554, Accuracy: 87.5, Test Loss: 0.18746420741081238, Test Accuracy: 94.45999908447266\n",
      "Epoch 147, Loss: 0.3097546398639679, Accuracy: 88.8888931274414, Test Loss: 0.2350943237543106, Test Accuracy: 94.30999755859375\n",
      "Epoch 148, Loss: 0.3575442135334015, Accuracy: 88.0859375, Test Loss: 0.20249681174755096, Test Accuracy: 94.81999969482422\n",
      "Epoch 149, Loss: 0.32110491394996643, Accuracy: 90.13671875, Test Loss: 0.16929657757282257, Test Accuracy: 95.4000015258789\n",
      "Epoch 150, Loss: 0.35563817620277405, Accuracy: 88.57421875, Test Loss: 0.15997961163520813, Test Accuracy: 95.27000427246094\n",
      "Epoch 151, Loss: 0.3778776526451111, Accuracy: 88.28125, Test Loss: 0.12270019948482513, Test Accuracy: 96.45999908447266\n",
      "Epoch 152, Loss: 0.3221236765384674, Accuracy: 90.33203125, Test Loss: 0.1853010058403015, Test Accuracy: 94.86000061035156\n",
      "Epoch 153, Loss: 0.3303118348121643, Accuracy: 89.6484375, Test Loss: 0.1574910581111908, Test Accuracy: 95.44000244140625\n",
      "Epoch 154, Loss: 0.300040066242218, Accuracy: 90.8203125, Test Loss: 0.14971591532230377, Test Accuracy: 95.77999877929688\n",
      "Epoch 155, Loss: 0.33254575729370117, Accuracy: 89.16015625, Test Loss: 0.23959165811538696, Test Accuracy: 93.20999908447266\n",
      "Epoch 156, Loss: 0.35272201895713806, Accuracy: 89.6484375, Test Loss: 0.19605891406536102, Test Accuracy: 94.3499984741211\n",
      "Epoch 157, Loss: 0.32051700353622437, Accuracy: 90.234375, Test Loss: 0.16425371170043945, Test Accuracy: 95.13999938964844\n",
      "Epoch 158, Loss: 0.3380114436149597, Accuracy: 88.96484375, Test Loss: 0.14755001664161682, Test Accuracy: 95.84000396728516\n",
      "Epoch 159, Loss: 0.373515784740448, Accuracy: 88.4765625, Test Loss: 0.2566644251346588, Test Accuracy: 93.58000183105469\n",
      "Epoch 160, Loss: 0.3103548586368561, Accuracy: 89.74609375, Test Loss: 0.16327962279319763, Test Accuracy: 95.4000015258789\n",
      "Epoch 161, Loss: 0.3720288574695587, Accuracy: 88.96484375, Test Loss: 0.14130254089832306, Test Accuracy: 96.06000518798828\n",
      "Epoch 162, Loss: 0.33325764536857605, Accuracy: 88.57421875, Test Loss: 0.15473298728466034, Test Accuracy: 95.52000427246094\n",
      "Epoch 163, Loss: 0.3872913122177124, Accuracy: 88.8671875, Test Loss: 0.17084059119224548, Test Accuracy: 95.02999877929688\n",
      "Epoch 164, Loss: 0.3053288459777832, Accuracy: 90.625, Test Loss: 0.19200751185417175, Test Accuracy: 94.25\n",
      "Epoch 165, Loss: 0.29099491238594055, Accuracy: 90.52734375, Test Loss: 0.2092893123626709, Test Accuracy: 93.75\n",
      "Epoch 166, Loss: 0.39471620321273804, Accuracy: 88.0859375, Test Loss: 0.13896465301513672, Test Accuracy: 95.87000274658203\n",
      "Epoch 167, Loss: 0.30831658840179443, Accuracy: 89.94140625, Test Loss: 0.24332845211029053, Test Accuracy: 93.52999877929688\n",
      "Epoch 168, Loss: 0.36019110679626465, Accuracy: 88.28125, Test Loss: 0.14302876591682434, Test Accuracy: 95.55000305175781\n",
      "Epoch 169, Loss: 0.29331091046333313, Accuracy: 90.625, Test Loss: 0.12982143461704254, Test Accuracy: 96.06999969482422\n",
      "Epoch 170, Loss: 0.29521623253822327, Accuracy: 91.015625, Test Loss: 0.12237870693206787, Test Accuracy: 96.4000015258789\n",
      "Epoch 171, Loss: 0.2941179871559143, Accuracy: 90.72265625, Test Loss: 0.1447545439004898, Test Accuracy: 95.67000579833984\n",
      "Epoch 172, Loss: 0.34009864926338196, Accuracy: 88.76953125, Test Loss: 0.20645323395729065, Test Accuracy: 94.20999908447266\n",
      "Epoch 173, Loss: 0.32614755630493164, Accuracy: 89.35546875, Test Loss: 0.345223069190979, Test Accuracy: 91.04999542236328\n",
      "Epoch 174, Loss: 0.3051950931549072, Accuracy: 90.0390625, Test Loss: 0.13478295505046844, Test Accuracy: 96.10000610351562\n",
      "Epoch 175, Loss: 0.30086013674736023, Accuracy: 90.91796875, Test Loss: 0.16164559125900269, Test Accuracy: 95.31000518798828\n",
      "Epoch 176, Loss: 0.29776638746261597, Accuracy: 90.77381134033203, Test Loss: 0.12516385316848755, Test Accuracy: 96.33000183105469\n",
      "Epoch 177, Loss: 0.31815776228904724, Accuracy: 89.16015625, Test Loss: 0.11694207042455673, Test Accuracy: 96.63999938964844\n",
      "Epoch 178, Loss: 0.3124067187309265, Accuracy: 90.33203125, Test Loss: 0.5065109729766846, Test Accuracy: 87.98999786376953\n",
      "Epoch 179, Loss: 0.31170758605003357, Accuracy: 90.234375, Test Loss: 0.14711150527000427, Test Accuracy: 95.80000305175781\n",
      "Epoch 180, Loss: 0.34214988350868225, Accuracy: 89.2578125, Test Loss: 0.13459530472755432, Test Accuracy: 95.99000549316406\n",
      "Epoch 181, Loss: 0.33529648184776306, Accuracy: 89.6484375, Test Loss: 0.1370072215795517, Test Accuracy: 96.24000549316406\n",
      "Epoch 182, Loss: 0.34470707178115845, Accuracy: 89.16015625, Test Loss: 0.14157257974147797, Test Accuracy: 95.77999877929688\n",
      "Epoch 183, Loss: 0.2672187387943268, Accuracy: 92.48046875, Test Loss: 0.13552767038345337, Test Accuracy: 96.06999969482422\n",
      "Epoch 184, Loss: 0.3498378396034241, Accuracy: 88.8671875, Test Loss: 0.1326970010995865, Test Accuracy: 95.99000549316406\n",
      "Epoch 185, Loss: 0.2975028157234192, Accuracy: 90.625, Test Loss: 0.1597883701324463, Test Accuracy: 95.19000244140625\n",
      "Epoch 186, Loss: 0.30857738852500916, Accuracy: 90.52734375, Test Loss: 0.13621072471141815, Test Accuracy: 95.85000610351562\n",
      "Epoch 187, Loss: 0.33725038170814514, Accuracy: 89.453125, Test Loss: 0.19100894033908844, Test Accuracy: 94.66999816894531\n",
      "Epoch 188, Loss: 0.3767126500606537, Accuracy: 88.57421875, Test Loss: 0.21814244985580444, Test Accuracy: 94.34000396728516\n",
      "Epoch 189, Loss: 0.3471021056175232, Accuracy: 89.6484375, Test Loss: 0.1360471099615097, Test Accuracy: 96.22000122070312\n"
     ]
    }
   ],
   "source": [
    "train_image_generator = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.35,\n",
    "        height_shift_range=0.35,\n",
    "        zoom_range=0.3,\n",
    "        preprocessing_function=add_noise)\n",
    " \n",
    "test_image_generator = ImageDataGenerator(rescale=1./255) \n",
    "final_test_loss = trainer(BasicCNN, train_image_generator, test_image_generator, \n",
    "                          verbose=True, max_epochs=300, batch_size = 64)\n",
    "print('Final test loss:', final_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of regularisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
